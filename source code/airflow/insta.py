##############
#DAG Setting
##############

import pandas as pd
import numpy as np
import re
import time
from kiwipiepy import Kiwi
# import tomotopy as tp
from pyspark.conf import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, IntegerType
from pyspark.sql.functions import array_contains, udf
from datetime import datetime
# Word2Vec embedding
from gensim.models import Word2Vec
from numpy import dot
from numpy.linalg import norm
import numpy as np
from sklearn.cluster import KMeans
# from textrank import KeywordSummarizer
from scipy.sparse import csr_matrix
# from soyclustering import SphericalKMeans

from airflow import DAG
from airflow.operators import PythonOperator
from datetime import datetime

dag = DAG(
        dag_id = "insta_fin",
        start_date = datetime(2021,10,2),
        schedule_interval = '@once'
    )

#############
#Python code
#############

def insta_call():
    import pandas as pd
    import subprocess
    from datetime import datetime

    import subprocess

    # í˜„ì¬ ì‹œê°„ ì„¤ì •
    nowtime = datetime.today().strftime("%Y-%m-%d")


    # daily_gamsung (1)
    daily_gamsung = pd.read_csv(f'/home/ubuntu/DE/instagram crawling to csv/+URL/daily_gamsung_{nowtime}.csv')
    daily_gamsung_DF = pd.DataFrame(daily_gamsung)
    daily_gamsung_DF.drop('Unnamed: 0', axis=1, inplace=True)
    daily_gamsung_DF.to_parquet(f'/home/ubuntu/DE/daily_gamsung_{nowtime}.parquet')

    # gamsung_bnb (2)
    gamsung_bnb = pd.read_csv(f'/home/ubuntu/DE/instagram crawling to csv/+URL/gamsung.bnb_{nowtime}.csv')
    gamsung_bnb_DF = pd.DataFrame(gamsung_bnb)
    gamsung_bnb_DF.drop('Unnamed: 0', axis=1, inplace=True)
    gamsung_bnb_DF.to_parquet(f'/home/ubuntu/DE/gamsung_bnb_{nowtime}.parquet')

    # gamsung_curation (3)
    gamsung_curation = pd.read_csv(f'/home/ubuntu/DE/instagram crawling to csv/+URL/gamsung_curation_{nowtime}.csv')
    gamsung_curation_DF = pd.DataFrame(gamsung_curation)
    gamsung_curation_DF.drop('Unnamed: 0', axis=1, inplace=True)
    gamsung_curation_DF.to_parquet(f'/home/ubuntu/DE/gamsung_curation_{nowtime}.parquet')

    # hi.stay.tour (4)
    hi_stay_tour = pd.read_csv(f'/home/ubuntu/DE/instagram crawling to csv/+URL/hi.stay.tour_{nowtime}.csv')
    hi_stay_tour_DF = pd.DataFrame(hi_stay_tour)
    hi_stay_tour_DF.drop('Unnamed: 0', axis=1, inplace=True)
    hi_stay_tour_DF.to_parquet(f'/home/ubuntu/DE/hi_stay_tour_{nowtime}.parquet')


    # rest_behappyhere (5)
    rest_behappyhere = pd.read_csv(f'/home/ubuntu/DE/instagram crawling to csv/+URL/rest_behappyhere_{nowtime}.csv')
    rest_behappyhere_DF = pd.DataFrame(rest_behappyhere)
    rest_behappyhere_DF.drop('Unnamed: 0', axis=1, inplace=True)
    rest_behappyhere_DF.to_parquet(f'/home/ubuntu/DE/rest_behappyhere_{nowtime}.parquet')

    # sookso.diary (6)
    sookso_diary = pd.read_csv(f'/home/ubuntu/DE/instagram crawling to csv/+URL/sookso.diary_{nowtime}.csv')
    sookso_diary_DF = pd.DataFrame(sookso_diary)
    sookso_diary_DF.drop('Unnamed: 0', axis=1, inplace=True)
    sookso_diary_DF.to_parquet(f'/home/ubuntu/DE/sookso_diary_{nowtime}.parquet')

    # sookso.hada (7)
    sookso_hada = pd.read_csv(f'/home/ubuntu/DE/instagram crawling to csv/+URL/sookso.hada_{nowtime}.csv')
    sookso_hada_DF = pd.DataFrame(sookso_hada)
    sookso_hada_DF.drop('Unnamed: 0', axis=1, inplace=True)
    sookso_hada_DF.to_parquet(f'/home/ubuntu/DE/sookso_hada_{nowtime}.parquet')

    # sukso_name = [daily_gamsung, gamsung_bnb, gamsung_curation, hi_stay_tour, rest_behappyhere, sookso_diary, sookso_hada]

    # for sukso in sukso_name:
    #     load_to_hadoop_script = "hdfs dfs -moveFromLocal {0} {1}".format('/home/ubuntu/DE/sukso_'+nowtime+ '.parquet', '/data/insta')
    
    # subprocess.call(load_to_hadoop_script,shell=True)




def daily_gamsung_extract(x):
    import re
    import numpy as np
    p = re.compile('\"[ê°€-í£\t\n\r\f\v\s\_ê°€-í£]+\"')
    m = p.findall(x)
    try:
        return m[0][1:-1]
    except:
        return np.nan

def gamsung_bnb_extract(x):
    import re
    import numpy as np
    p = re.compile("[\'|\"][ê°€-í£]+[\'|\"]") 
    m = p.findall(x)
    try:
        return m[0][1:-1]
    except :
        return np.nan

def gamsung_curation_extract(x):
    import re
    import numpy as np
    p = re.compile('\ğŸ“[ê°€-í£]+') # 
    m = p.findall(x)
    try:
        return m[0][1:] 
    except :
        return ''

def hi_stay_tour_extract(x):
    import re
    import numpy as np
    p = re.compile("\#[ê°€-í£]+") # 
    m = p.findall(x)
    try:
        return m[0][1:] 
    except :
        return ''

def rest_behappyhere_extract(x):
    import re
    import numpy as np
    p = re.compile('\#[ê°€-í£]+')
    m = p.findall(x)
    try:
        return m[1][1:]
    except:
        return np.nan

def sookso_diary_extract(x):
    import re
    import numpy as np
    p = re.compile('\ğŸ“ [ê°€-í£]+\s[ê°€-í‡]+|\ğŸ“[ê°€-í£]+\s[ê°€-í‡]+')
    m = p.findall(x)
    try:
        return m[0].split(' ')[-1]
    except:
        return np.nan

def sookso_hada_extract(x):
    import re
    import numpy as np
    p = re.compile('\#[ê°€-í£]+')
    m = p.findall(x)
    if  m[1] == '#ìˆ™ì†Œí•˜ë‹¤': m[1], m[0] = m[0], m[1]
    return m[1][1:]


def daily_gamsung_extract(x):
    import re
    import numpy as np
    p = re.compile('\"[ê°€-í£\t\n\r\f\v\s\_ê°€-í£]+\"')
    m = p.findall(x)
    try:
        return m[0][1:-1]
    except:
        return np.nan

# def convert(x):
#     return int(x.replace(',',''))

def convert(x):
    if x:
        return int(x.replace(',',''))
    else:
        return 0

#####################################################
### daily_gamsung(ì™„ë£Œ)    
def insta_spark_processing():
    from pyspark.sql.types import StructType,StructField, StringType, IntegerType
    from pyspark.sql.functions import array_contains, udf
    from pyspark.conf import SparkConf
    from pyspark.sql import SparkSession


    from datetime import datetime, timedelta
    from pymongo import MongoClient
    import pandas as pd
    import numpy as np
    from difflib import SequenceMatcher
    from collections import Counter
    from datetime import datetime
    import re

    import time
    import os
    import sys
    import urllib.request
    import json

    nowtime = datetime.today().strftime("%Y-%m-%d")#  - pd.DateOffset(days=1)
    spark = SparkSession\
            .builder\
            .appName('Python Spark SQL basic example')\
            .getOrCreate()

    sc = spark.sparkContext
    
    # pymongo connect
    client = MongoClient('localhost',27017) # mongodb 27017 port
    db = client.ojo_db
    cur = db['cluster']

    coronaStage = spark.read.parquet(f"hdfs://localhost:9000/data/corona/coronaStage_2021-10-04")

    daily_gamsung_DFP = spark.read.parquet(f"hdfs://localhost:9000/data/insta/daily_gamsung_{nowtime}.parquet")
    daily_gamsung_DFP = daily_gamsung_DFP.drop('Unnamed: 0','unix')
    daily_gamsung = daily_gamsung_DFP.toPandas()

    gamsung_bnb_DFP = spark.read.parquet(f"hdfs://localhost:9000/data/insta/gamsung_bnb_{nowtime}.parquet")
    gamsung_bnb_DFP = gamsung_bnb_DFP.drop('Unnamed: 0','unix')
    gamsung_bnb = gamsung_bnb_DFP.toPandas()

    gamsung_curation_DFP = spark.read.parquet(f"hdfs://localhost:9000/data/insta/gamsung_curation_{nowtime}.parquet")
    gamsung_curation_DFP = gamsung_curation_DFP.drop('Unnamed: 0','unix')
    gamsung_curation = gamsung_curation_DFP.toPandas()

    hi_stay_tour_DFP = spark.read.parquet(f"hdfs://localhost:9000/data/insta/hi_stay_tour_{nowtime}.parquet")
    hi_stay_tour_DFP = hi_stay_tour_DFP.drop('Unnamed: 0','unix')
    hi_stay_tour = hi_stay_tour_DFP.toPandas()

    rest_behappyhere_DFP = spark.read.parquet(f"hdfs://localhost:9000/data/insta/rest_behappyhere_{nowtime}.parquet")
    rest_behappyhere_DFP = rest_behappyhere_DFP.drop('Unnamed: 0','unix')
    rest_behappyhere = rest_behappyhere_DFP.toPandas()

    sookso_diary_DFP = spark.read.parquet(f"hdfs://localhost:9000/data/insta/sookso_diary_{nowtime}.parquet")
    sookso_diary_DFP = sookso_diary_DFP.drop('Unnamed: 0','unix')
    sookso_diary = sookso_diary_DFP.toPandas()

    sookso_hada_DFP = spark.read.parquet(f"hdfs://localhost:9000/data/insta/sookso_hada_{nowtime}.parquet")
    sookso_hada_DFP = sookso_hada_DFP.drop('Unnamed: 0','unix')
    sookso_hada = sookso_hada_DFP.toPandas()
    
    #######################################
    daily_gamsung_name = daily_gamsung.content.apply(lambda x : daily_gamsung_extract(x))
    daily_gamsung['name'] = daily_gamsung_name
    daily_gamsung = daily_gamsung[~(daily_gamsung['name'].isna())]

    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°
    indexNames = daily_gamsung[daily_gamsung['name'] == ''].index
    daily_gamsung.drop(indexNames, inplace=True) 

    if daily_gamsung.like.dtype == 'O':
        daily_gamsung.like = daily_gamsung.like.apply(lambda x : convert(x))


    # 1,2,3
    dg_cnt = Counter(daily_gamsung.name)
    li =[]
    for name, cnt  in dg_cnt.items():
        if cnt > 1:
            li.append(name)   

    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append
    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ
        ex = daily_gamsung.loc[daily_gamsung.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤
        ex.reset_index(drop=True,inplace=True)     # index reset

        content = ex.iloc[0,:].content
        date = ex.iloc[0,:].date
        like = ex.iloc[0,:].like
        place = ex.iloc[0,:].place
        tags = ex.iloc[0,:].tags
        imgUrl = ex.iloc[0,:].imgUrl
        name = ex.name[0]

        for i in range(len(ex)-1):
            like += ex.iloc[i+1,:].like
            tags += ex.iloc[i+1,:].tags

        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°
        indexNames = daily_gamsung[daily_gamsung['name'] == ex.name[0]].index
        daily_gamsung.drop(indexNames, inplace=True) 
        daily_gamsung = daily_gamsung.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=daily_gamsung.columns),ignore_index=True)

    dg_tmp = []
    for i in daily_gamsung.name:
        dg_tmp.append(dg_cnt[i]-1)

    daily_gamsung['overlap'] = dg_tmp


    #####################################################
    ### gamsun.bnb
    gamsung_bnb_name = gamsung_bnb.content.apply(lambda x : gamsung_bnb_extract(x)) # gamsung_bnb_name = 
    gamsung_bnb['name'] = gamsung_bnb_name
    gamsung_bnb = gamsung_bnb[~(gamsung_bnb['name'].isna())]

    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°
    indexNames = gamsung_bnb[gamsung_bnb['name'] == ''].index
    gamsung_bnb.drop(indexNames, inplace=True) 

    # like integet ë³€í™˜ í›„ í•©ì¹˜ê¸°

    if gamsung_bnb.like.dtype == 'O':
        gamsung_bnb.like = gamsung_bnb.like.apply(lambda x : convert(x))



    # 1, ì¤‘ë³µëœ ì´ë¦„ ê²€ìƒ‰ 
    # 2.ê°œìˆ˜ê°€ 1ì¸ê±° ì´ìƒ ë½‘ì•„ë‚´ê¸° 
    # 3.ë¦¬ìŠ¤íŠ¸ë§Œë“¤ê¸°
    gb_cnt = Counter(gamsung_bnb.name)      # Counter ëª¨ë“ˆë¡œ name data count
    li =[]
    for name, cnt  in gb_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)
        if cnt > 1:
            li.append(name)

    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append
    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ
        ex = gamsung_bnb.loc[gamsung_bnb.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤
        ex.reset_index(drop=True,inplace=True)     # index reset

        content = ex.iloc[0,:].content
        date = ex.iloc[0,:].date
        like = ex.iloc[0,:].like
        place = ex.iloc[0,:].place
        tags = ex.iloc[0,:].tags
        imgUrl = ex.iloc[0,:].imgUrl
        name = ex.name[0]

        for i in range(len(ex)-1):
            like += ex.iloc[i+1,:].like
            tags += ex.iloc[i+1,:].tags


        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°
        indexNames = gamsung_bnb[gamsung_bnb['name'] == ex.name[0]].index
        gamsung_bnb.drop(indexNames, inplace=True) 
        gamsung_bnb = gamsung_bnb.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=gamsung_bnb.columns),ignore_index=True)

    gb_tmp = []
    for i in gamsung_bnb.name:
        gb_tmp.append(gb_cnt[i]-1)


    gamsung_bnb['overlap'] = gb_tmp


    #####################################################
    ### gamsung_curation(ì™„ë£Œ)
    gamsung_curation_name = gamsung_curation.content.apply(lambda x : gamsung_curation_extract(x))
    gamsung_curation['name'] = gamsung_curation_name
    gamsung_curation = gamsung_curation[~(gamsung_curation['name'].isna())]


    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°
    indexNames = gamsung_curation[gamsung_curation['name'] == ''].index
    gamsung_curation.drop(indexNames, inplace=True) 




    if gamsung_curation.like.dtype == 'O':
        gamsung_curation.like = gamsung_curation.like.apply(lambda x : convert(x))


    gc_cnt = Counter(gamsung_curation.name)      # Counter ëª¨ë“ˆë¡œ name data count
    li =[]
    for name, cnt  in gc_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)
        if cnt > 1:
            li.append(name)        


    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append
    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ
        ex = gamsung_curation.loc[gamsung_curation.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤
        ex.reset_index(drop=True,inplace=True)     # index reset

        content = ex.iloc[0,:].content
        date = ex.iloc[0,:].date
        like = ex.iloc[0,:].like
        place = ex.iloc[0,:].place
        tags = ex.iloc[0,:].tags
        imgUrl = ex.iloc[0,:].imgUrl
        name = ex.name[0]

        for i in range(len(ex)-1):
            like += ex.iloc[i+1,:].like
            tags += ex.iloc[i+1,:].tags

        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°
        indexNames = gamsung_curation[gamsung_curation['name'] == ex.name[0]].index
        gamsung_curation.drop(indexNames, inplace=True) 
        gamsung_curation = gamsung_curation.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=gamsung_curation.columns),ignore_index=True)

    gc_tmp = []
    for i in gamsung_curation.name:
        gc_tmp.append(gc_cnt[i]-1)

    gamsung_curation['overlap'] = gc_tmp

    #####################################################
    ### hi.stay.tour.csv (ì™„ë£Œ)
    hi_stay_tour_name = hi_stay_tour.content.apply(lambda x : hi_stay_tour_extract(x))
    hi_stay_tour['name'] = hi_stay_tour_name
    hi_stay_tour = hi_stay_tour[~(hi_stay_tour['name'].isna())]


    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°
    indexNames = hi_stay_tour[hi_stay_tour['name'] == ''].index
    hi_stay_tour.drop(indexNames, inplace=True) 


    if hi_stay_tour.like.dtype == 'O':
        hi_stay_tour.like = hi_stay_tour.like.apply(lambda x : convert(x))


    # 1, ì¤‘ë³µëœ ì´ë¦„ ê²€ìƒ‰ 
    # 2.ê°œìˆ˜ê°€ 1ì¸ê±° ì´ìƒ ë½‘ì•„ë‚´ê¸° 
    # 3.ë¦¬ìŠ¤íŠ¸ë§Œë“¤ê¸°
    hst_cnt = Counter(hi_stay_tour.name)      # Counter ëª¨ë“ˆë¡œ name data count
    li =[]
    for name, cnt  in hst_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)
        if cnt > 1:
            li.append(name)  

    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append
    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ
        ex = hi_stay_tour.loc[hi_stay_tour.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤
        ex.reset_index(drop=True,inplace=True)     # index reset

        content = ex.iloc[0,:].content
        date = ex.iloc[0,:].date
        like = ex.iloc[0,:].like
        place = ex.iloc[0,:].place
        tags = ex.iloc[0,:].tags
        imgUrl = ex.iloc[0,:].imgUrl
        name = ex.name[0]

        for i in range(len(ex)-1):
            like += ex.iloc[i+1,:].like
            tags += ex.iloc[i+1,:].tags



        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°
        indexNames = hi_stay_tour[hi_stay_tour['name'] == ex.name[0]].index
        hi_stay_tour.drop(indexNames, inplace=True) 
        hi_stay_tour = hi_stay_tour.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=hi_stay_tour.columns),ignore_index=True)

    hst_cnt_tmp = []
    for i in hi_stay_tour.name:
        hst_cnt_tmp.append(hst_cnt[i]-1)

    hi_stay_tour['overlap'] = hst_cnt_tmp


    #####################################################
    ### rest_behappyhere.csv
    rest_behappyhere_name = rest_behappyhere.content.apply(lambda x : rest_behappyhere_extract(x))
    rest_behappyhere['name'] = rest_behappyhere_name
    rest_behappyhere = rest_behappyhere[~(rest_behappyhere['name'].isna())]

    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°
    indexNames = rest_behappyhere[rest_behappyhere['name'] == ''].index
    rest_behappyhere.drop(indexNames, inplace=True) 



    if rest_behappyhere.like.dtype == 'O':
        rest_behappyhere.like = rest_behappyhere.like.apply(lambda x : convert(x))


    # 1, ì¤‘ë³µëœ ì´ë¦„ ê²€ìƒ‰ 
    # 2.ê°œìˆ˜ê°€ 1ì¸ê±° ì´ìƒ ë½‘ì•„ë‚´ê¸° 
    # 3.ë¦¬ìŠ¤íŠ¸ë§Œë“¤ê¸°
    rb_cnt = Counter(rest_behappyhere.name)      # Counter ëª¨ë“ˆë¡œ name data count
    li =[]
    for name, cnt  in rb_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)
        if cnt > 1:
            li.append(name)


    #####################################################
    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append
    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ
        ex = rest_behappyhere.loc[rest_behappyhere.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤
        ex.reset_index(drop=True,inplace=True)     # index reset

        content = ex.iloc[0,:].content
        date = ex.iloc[0,:].date
        like = ex.iloc[0,:].like
        place = ex.iloc[0,:].place
        tags = ex.iloc[0,:].tags
        imgUrl = ex.iloc[0,:].imgUrl
        name = ex.name[0]

        for i in range(len(ex)-1):
            like += ex.iloc[i+1,:].like
            tags += ex.iloc[i+1,:].tags



        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°
        indexNames = rest_behappyhere[rest_behappyhere['name'] == ex.name[0]].index
        rest_behappyhere.drop(indexNames, inplace=True) 
        rest_behappyhere = rest_behappyhere.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=rest_behappyhere.columns),ignore_index=True)

    rb_cnt_tmp = []
    for i in rest_behappyhere.name:
        rb_cnt_tmp.append(rb_cnt[i]-1)

    rest_behappyhere['overlap'] = rb_cnt_tmp

    #####################################################
    ### sookso.diary(ì™„ë£Œ)
    sookso_diary_name = sookso_diary.content.apply(lambda x : sookso_diary_extract(x))
    sookso_diary['name'] = sookso_diary_name
    sookso_diary = sookso_diary[~(sookso_diary['name'].isna())]


    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°
    indexNames = sookso_diary[sookso_diary['name'] == ''].index
    sookso_diary.drop(indexNames, inplace=True) 



    if sookso_diary.like.dtype == 'O':
        sookso_diary.like = sookso_diary.like.apply(lambda x : convert(x))


    # 1, ì¤‘ë³µëœ ì´ë¦„ ê²€ìƒ‰ 
    # 2.ê°œìˆ˜ê°€ 1ì¸ê±° ì´ìƒ ë½‘ì•„ë‚´ê¸° 
    # 3.ë¦¬ìŠ¤íŠ¸ë§Œë“¤ê¸°
    sd_cnt = Counter(sookso_diary.name)      # Counter ëª¨ë“ˆë¡œ name data count
    li =[]
    for name, cnt  in sd_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)
        if cnt > 1:
            li.append(name)

    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append
    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ
        ex = sookso_diary.loc[sookso_diary.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤
        ex.reset_index(drop=True,inplace=True)     # index reset

        content = ex.iloc[0,:].content
        date = ex.iloc[0,:].date
        like = ex.iloc[0,:].like
        place = ex.iloc[0,:].place
        tags = ex.iloc[0,:].tags
        imgUrl = ex.iloc[0,:].imgUrl
        name = ex.name[0]

        for i in range(len(ex)-1):
            like += ex.iloc[i+1,:].like
            tags += ex.iloc[i+1,:].tags



        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°
        indexNames = sookso_diary[sookso_diary['name'] == ex.name[0]].index
        sookso_diary.drop(indexNames, inplace=True) 
        sookso_diary = sookso_diary.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=sookso_diary.columns),ignore_index=True)

    sd_cnt_tmp = []
    for i in sookso_diary.name:
        sd_cnt_tmp.append(sd_cnt[i]-1)

    sookso_diary['overlap'] = sd_cnt_tmp

    #####################################################

    ### sookso.hada (ì™„ë£Œ)
    sookso_hada_name = sookso_hada.content.apply(lambda x : sookso_hada_extract(x))
    sookso_hada['name'] = sookso_hada_name
    sookso_hada = sookso_hada[~(sookso_hada['name'].isna())]

    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°
    indexNames = sookso_hada[sookso_hada['name'] == ''].index
    sookso_hada.drop(indexNames, inplace=True) 


    # like integet ë³€í™˜ í›„ í•©ì¹˜ê¸°

    if sookso_hada.like.dtype == 'O':
        sookso_hada.like = sookso_hada.like.apply(lambda x : convert(x))


    # 1, ì¤‘ë³µëœ ì´ë¦„ ê²€ìƒ‰ 
    # 2.ê°œìˆ˜ê°€ 1ì¸ê±° ì´ìƒ ë½‘ì•„ë‚´ê¸° 
    # 3.ë¦¬ìŠ¤íŠ¸ë§Œë“¤ê¸°
    sh_cnt = Counter(sookso_hada.name)      # Counter ëª¨ë“ˆë¡œ name data count
    li =[]
    for name, cnt  in sh_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)
        if cnt > 1:
            li.append(name)        


    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append
    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ
        ex = sookso_hada.loc[sookso_hada.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤
        ex.reset_index(drop=True,inplace=True)     # index reset

        content = ex.iloc[0,:].content
        date = ex.iloc[0,:].date
        like = ex.iloc[0,:].like
        place = ex.iloc[0,:].place
        tags = ex.iloc[0,:].tags
        imgUrl = ex.iloc[0,:].imgUrl
        name = ex.name[0]

        for i in range(len(ex)-1):
            like += ex.iloc[i+1,:].like
            tags += ex.iloc[i+1,:].tags

        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°
        indexNames = sookso_hada[sookso_hada['name'] == ex.name[0]].index
        sookso_hada.drop(indexNames, inplace=True) 
        sookso_hada = sookso_hada.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=sookso_hada.columns),ignore_index=True)

    sh_cnt_tmp = []
    for i in sookso_hada.name:
        sh_cnt_tmp.append(sh_cnt[i]-1)

    sookso_hada['overlap'] = sh_cnt_tmp

    ######################################
    ##  ë°ì´í„° í”„ë ˆì„ í•©ì¹˜ê¸°
    tot_dataset = daily_gamsung
    for df in [gamsung_bnb, gamsung_curation, hi_stay_tour, rest_behappyhere, sookso_diary, sookso_hada]:
        tot_dataset = tot_dataset.append(df)

    if tot_dataset.like.dtype == 'O':
        tot_dataset.like = tot_dataset.like.apply(lambda x : convert(x))



    dg_cnt = Counter(tot_dataset.name)      # Counter ëª¨ë“ˆë¡œ name data count
    li =[]
    for name, cnt  in dg_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)
        if cnt > 1:
            li.append(name)


    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append
    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ
        ex = tot_dataset.loc[tot_dataset.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤
        ex.reset_index(drop=True,inplace=True)     # index reset

        content = ex.iloc[0,:].content
        date = ex.iloc[0,:].date
        like = ex.iloc[0,:].like
        place = ex.iloc[0,:].place
        tags = ex.iloc[0,:].tags
        imgUrl = ex.iloc[0,:].imgUrl
        name = ex.name[0]
        overlap = ex.iloc[0,:].overlap

        for i in range(len(ex)-1):
            like += ex.iloc[i+1,:].like
            tags += ex.iloc[i+1,:].tags
            overlap += ex.iloc[i+1,:].overlap


        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°
        indexNames = tot_dataset[tot_dataset['name'] == ex.name[0]].index
        tot_dataset.drop(indexNames, inplace=True) 
        tot_dataset = tot_dataset.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name,overlap],index=tot_dataset.columns),ignore_index=True)

    ####################################
    ## Naver ì¥ì†Œ API

    # naver application clientID,secret
    client_id = "6E6mBjvxbdEgVk6Prqgd"
    client_secret = "0ZjWnYiBjr"


    # quoteê°€ ì¸ì½”ë”©ì„ ë³€ê²½(UTF8ë¡œ urlì— ë„˜ê²¨ì£¼ê¸° ìœ„í•´ì„œ)
    # ë‚´ê°€ ì…ë ¥í•œ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ parse
    title_et = []
    for name in tot_dataset.name:
        query = urllib.parse.quote(f'ìˆ™ì†Œ+" "+{name}')
        idx = 0 # ìš”ì²­ ê°œìˆ˜
        display = 10 # 100ê°œ ë‹¨ìœ„ë¡œ ê°€ì ¸ì˜¨ë‹¤
        start = 1
        end = 100
        tmp=''

        url = "https://openapi.naver.com/v1/search/local?query=" + query \
            + "&display=" + str(display) \
            + "&start=" + str(start)


        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id",client_id)
        request.add_header("X-Naver-Client-Secret",client_secret)
        # request ìš”ì²­ì— ëŒ€í•œ ê²°ê³¼ë¥¼ responseì— ì €ì¥
        response = urllib.request.urlopen(request)
        rescode = response.getcode()
        if(rescode==200):
            response_body = response.read()
            response_dict = json.loads(response_body.decode('utf-8'))
            items = response_dict['items']
            for item_index in range(0,len(items)):
                remove_tag = re.compile('<.*?>')
                title = re.sub(remove_tag, '', items[item_index]['title'])
                link = items[item_index]['link']
                caterory = items[item_index]['category']
                description = re.sub(remove_tag, '',items[item_index]['description'])
                telephone = items[item_index]['telephone']
                address = items[item_index]['address']
                roadAddress = items[item_index]['roadAddress']
                mapx = items[item_index]['mapx']
                mapy = items[item_index]['mapy']

                # ì´ë¦„ì´ ì—†ëŠ” ê²½ìš°
                if not title :
                    title_et.append('')
                else: #'ì „í†µìˆ™ì†Œ' 'í˜¸ìŠ¤í…”''ë¦¬ì¡°íŠ¸''ë¦¬ì¡°íŠ¸ë¶€ì†ê±´ë¬¼'
                    if caterory[:2] in ['ìˆ™ë°•'] :  # ì¹´í…Œê³ ë¦¬ê°€ ìˆ™ë°•ì¸ ê²½ìš°
                        title = title.replace(' ','')
                        name = name.replace(' ','')
                        ratio = SequenceMatcher(None, title, name).ratio()
                        if ratio >= 0.7 : # title.replace(' ','') in name.replace(' ','')
                            tmp = address

            if tmp : title_et.append(tmp)
            else: title_et.append('')

        else:
            print("Error Code:" + rescode)

        time.sleep(0.1)


    tot_dataset.drop('place',axis=1,inplace=True)
    tot_dataset['place'] = title_et

    #####################################
    ### ì½”ë¡œë‚˜ ë‹¨ê³„ ì‘ì—…
    coronaStage = coronaStage.toPandas() 

    stage =[str() for i in range(len(tot_dataset))]
    local = [str() for i in range(len(tot_dataset))]
    for lo in coronaStage.Area.unique().tolist():
        for j in range(len(tot_dataset)):
            try :
                com = tot_dataset.iloc[j].place.split()[0]
                if list(lo)[0] in com and list(lo)[1] in com:
                    stage[j] = coronaStage[coronaStage.Area == lo].Stage.values[0]
                    local[j] = lo
            except:
                pass

    tot_dataset['stage'] = stage
    tot_dataset['local'] = local

    ############################
    ## ìƒˆë¡œìš´ ìˆ™ì†Œ ìƒì„±


    ch = []
    for i in cur.find():
        ch.append(i)
    change = pd.DataFrame(ch)

    a = tot_dataset.merge(change,how='left' ,on='name')
    li = a[a.tags_y.isnull() == True].index.tolist()
    new = tot_dataset.iloc[li]
    print(new)
#     new.drop('index',axis=1,inplace=True)
    new.to_parquet(f'/home/ubuntu/DE/new_{nowtime}.parquet')

def load_to_hdfs():
    import subprocess
    # move file to hdfs.
    nowtime = datetime.today().strftime("%Y-%m-%d")
    load_to_hadoop_script1 = "hdfs dfs -moveFromLocal {0} {1}".format('/home/ubuntu/DE/daily_gamsung_' + nowtime + '.parquet', '/data/insta')
    load_to_hadoop_script2 = "hdfs dfs -moveFromLocal {0} {1}".format('/home/ubuntu/DE/gamsung_bnb_' + nowtime + '.parquet', '/data/insta')
    load_to_hadoop_script3 = "hdfs dfs -moveFromLocal {0} {1}".format('/home/ubuntu/DE/gamsung_curation_' + nowtime + '.parquet', '/data/insta')
    load_to_hadoop_script4 = "hdfs dfs -moveFromLocal {0} {1}".format('/home/ubuntu/DE/hi_stay_tour_' + nowtime + '.parquet', '/data/insta')
    load_to_hadoop_script5 = "hdfs dfs -moveFromLocal {0} {1}".format('/home/ubuntu/DE/rest_behappyhere_' + nowtime + '.parquet', '/data/insta')
    load_to_hadoop_script6 = "hdfs dfs -moveFromLocal {0} {1}".format('/home/ubuntu/DE/sookso_diary_' + nowtime + '.parquet', '/data/insta')
    load_to_hadoop_script7 = "hdfs dfs -moveFromLocal {0} {1}".format('/home/ubuntu/DE/sookso_hada_' + nowtime + '.parquet', '/data/insta')
    subprocess.call(load_to_hadoop_script1,shell=True)
    subprocess.call(load_to_hadoop_script2,shell=True)
    subprocess.call(load_to_hadoop_script3,shell=True)
    subprocess.call(load_to_hadoop_script4,shell=True)
    subprocess.call(load_to_hadoop_script5,shell=True)
    subprocess.call(load_to_hadoop_script6,shell=True)
    subprocess.call(load_to_hadoop_script7,shell=True)



def newdata_to_hdfs():
    import subprocess
    # move file to hdfs.
    nowtime = datetime.today().strftime("%Y-%m-%d")
    load_to_hadoop_script1 = "hdfs dfs -moveFromLocal {0} {1}".format('/home/ubuntu/DE/new_' + nowtime + '.parquet', '/data/modeldata')
    subprocess.call(load_to_hadoop_script1,shell=True)
    




def token(docs):
    return docs

def insta_preporcessing(insta):
    # ë°ì´í„° ë¡œë“œ
    insta_data = pd.read_csv(insta)

    # 1ì°¨ í´ë¦°ì§•(ì˜ì–´, íŠ¹ìˆ˜ë¬¸ì, ìˆ«ìì œê±°)
    for i in range(len(insta_data.content)):
        insta_data.content[i] = sub_special(insta_data.content[i])

    # í† í°í™”
    for i in range(len(insta_data.content)):
        insta_data.content[i]= tokenize(insta_data.content[i])

    # 2ì°¨ í´ë¦°ì§•(ë¶ˆìš©ì–´ ì²˜ë¦¬)
        word_cleansing(insta_data)

    return insta_data


def naver_preprocessing(naver):
    #ë°ì´í„° ë¡œë“œ
    naver_data = pd.read_csv(naver)

    # ì¸ë±ìŠ¤ ì œê±°
    naver_data.drop('Unnamed: 0', axis=1, inplace=True)

    # í† í°í™”
    for i in range(len(naver_data)):
        naver_data.content[i] = sub_special_token(naver_data.content[i])

    #í´ë¦°ì§•
    word_cleansing(naver_data)

    return naver_data

def make_stopwords(input_text):
    with open (input_text, 'r', encoding= 'utf-8') as f:
        words = f.readline().split()
    stop_words = 'ì˜ ê°€ ì´ ì€ ë“¤ ëŠ” ì¢€ ì˜ ì†ì´ˆ ê± ê³¼ í–ˆ ê±° í•´ì„œ ìˆ™ë°• ê²½ê¸°ë„ ë– ë‚˜ë‹¤ ëŠë¼ë‹¤ ë¹„ì•  ëŒ“ê¸€ ê¶Œ ë§Œì› ë¦¬ì¡°íŠ¸ ì¹´ë¼ ì¶”ì²¨ ì¦ê¸°ë‹¤ ë‹¤ë…€ì˜¤ë‹¤ ì¶œì²˜ ì—¬í–‰ ì œê³µ ê°ì‚¬ ê²Œ ì° ëŠë‚Œ ë§ì´ ë“¯ ë·° ë°• ë§í¬ ì¸ìŠ¤íƒ€ê·¸ë¨ í• ì¸ ì„±ìˆ˜ê¸° í™ˆí˜ì´ì§€ ì£¼ë§ ë¸”ë¡œê·¸ ì¸ì› ì¶”ê°€ ã…ã… ë„ˆë¬´ ê²ŒìŠ¤íŠ¸ ë„˜ í•˜ìš°ìŠ¤ ë“œë¦¬ë‹¤ ì´ìš© ìœ„ì¹˜ ì“°ë‹¤ ì§„ì§œ ë„˜ ì° ê±° ë¨¹ ã… ã…  ã…ã…ã… ã…  ë¬¼ ì˜€ ã… ã… ã…  ã… ã…  ã…  ã…‹ã…‹ã…‹ ã…‹ã…‹ ã…‹ ã…ã…ã… ã…ã… ã… í™”ì¥ì‹¤ ë„ ë­ ì˜¤í”ˆ ìµœëŒ€ ì¤€ë¹„ ë£¸ ë¹µ ê±° ë§ì´ ë°©ë²• ë‹¬ë¦¬ ìŠ¤ëŸ½ë‹¤ íŠ¹ë³„ìì¹˜ë„ ë¥¼ ì—ì–´ë¹„ ìœ¼ë¡œ ì ì— ì™€ í•œ í•˜ë‹¤ ì•„ íœ´ ì•„ì´êµ¬ í¬ìŠ¤íŒ… ì•„ì´ì¿  ì•„ì´ê³  ì–´ ë‚˜ ìš°ë¦¬ ì €í¬ ë”°ë¼ ì˜í•´ ì„ ë¥¼ ì— ì˜ ê°€ ìœ¼ë¡œ ë¡œ ì—ê²Œ ë¿ì´ë‹¤ ì˜ê±°í•˜ì—¬ ê·¼ê±°í•˜ì—¬ ì…ê°í•˜ì—¬ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆí•˜ë©´ ì˜ˆë¥¼ ë“¤ë©´ ì˜ˆë¥¼ ë“¤ìë©´ ì € ì†Œì¸ ì†Œìƒ ì €í¬ ì§€ë§ê³  í•˜ì§€ë§ˆ í•˜ì§€ë§ˆë¼ ë‹¤ë¥¸ ë¬¼ë¡  ë˜í•œ ê·¸ë¦¬ê³  ë¹„ê¸¸ìˆ˜ ì—†ë‹¤ í•´ì„œëŠ” ì•ˆëœë‹¤ ë¿ë§Œ ì•„ë‹ˆë¼ ë§Œì´ ì•„ë‹ˆë‹¤ ë§Œì€ ì•„ë‹ˆë‹¤ ë§‰ë¡ í•˜ê³  ê´€ê³„ì—†ì´ ê·¸ì¹˜ì§€ ì•Šë‹¤ ê·¸ëŸ¬ë‚˜ ê·¸ëŸ°ë° í•˜ì§€ë§Œ ë“ ê°„ì— ë…¼í•˜ì§€ ì•Šë‹¤ ë”°ì§€ì§€ ì•Šë‹¤ ì„¤ì‚¬ ë¹„ë¡ ë”ë¼ë„ ì•„ë‹ˆë©´ ë§Œ ëª»í•˜ë‹¤ í•˜ëŠ” í¸ì´ ë‚«ë‹¤ ë¶ˆë¬¸í•˜ê³  í–¥í•˜ì—¬ í–¥í•´ì„œ í–¥í•˜ë‹¤ ìª½ìœ¼ë¡œ í‹ˆíƒ€ ì´ìš©í•˜ì—¬ íƒ€ë‹¤ ì˜¤ë¥´ë‹¤ ì œì™¸í•˜ê³  ì´ ì™¸ì— ì´ ë°–ì— í•˜ì—¬ì•¼ ë¹„ë¡œì†Œ í•œë‹¤ë©´ ëª°ë¼ë„ ì™¸ì—ë„ ì´ê³³ ì—¬ê¸° ë¶€í„° ê¸°ì ìœ¼ë¡œ ë”°ë¼ì„œ í•  ìƒê°ì´ë‹¤ í•˜ë ¤ê³ í•˜ë‹¤ ì´ë¦¬í•˜ì—¬ ê·¸ë¦¬í•˜ì—¬ ê·¸ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ í•˜ì§€ë§Œ ì¼ë•Œ í• ë•Œ ì•ì—ì„œ ì¤‘ì—ì„œ ë³´ëŠ”ë°ì„œ ìœ¼ë¡œì¨ ë¡œì¨ ê¹Œì§€ í•´ì•¼í•œë‹¤ ì¼ê²ƒì´ë‹¤ ë°˜ë“œì‹œ í• ì¤„ì•Œë‹¤ í• ìˆ˜ìˆë‹¤ í• ìˆ˜ìˆì–´ ì„ì— í‹€ë¦¼ì—†ë‹¤ í•œë‹¤ë©´ ë“± ë“±ë“± ì œ ê²¨ìš° ë‹¨ì§€ ë‹¤ë§Œ í• ë¿ ë”©ë™ ëŒ•ê·¸ ëŒ€í•´ì„œ ëŒ€í•˜ì—¬ ëŒ€í•˜ë©´ í›¨ì”¬ ì–¼ë§ˆë‚˜ ì–¼ë§ˆë§Œí¼ ì–¼ë§ˆí¼ ë‚¨ì§“ ì´ì œ ë¶„ ë„ì›€ ì—¬ ã… ã… ã„¶ ì–¼ë§ˆê°„ ë‘¥ ì˜¤ëœë§Œ ì•½ê°„ ì²´í¬ ì²´í¬ì•„ì›ƒ ê°€ê²© ì •ë³´ ë¹„ ìˆ˜ê¸° í‰ì¼ ê¸°ì¤€ ì•½ ì „êµ­  ì˜ˆì•½ ë˜ì–´ë‹¤ ìŠ¤ë§ˆíŠ¸ ë¹” ë¸”ë£¨íˆ¬ìŠ¤ ìŠ¤í”¼ì»¤ ì´ ì™¸ ë“œë¼ì´ì–´ ë‹¤ë¦¬ë¯¸ êµ¬ë¹„ ë˜ì–´ë‹¤ ìˆë‹¤ ì£¼ë°© ìŒì‹ ì¡°ë¦¬ ê°€ëŠ¥í•˜ë‹¤ í™˜ê¸° ì œí•œ ì „ë‚¨ ì œë¶€ë„ ìˆë‹¤ ëƒ„ìƒˆ ë‚˜ ìš”ë¦¬ ì‚¼ê°€ë‹¤ ê²½ì£¼ì‹œ ë¶€íƒë“œë¦¬ë‹¤ í›„ ë²ˆê¸¸ ë¬¸ì˜ ë‹¤ ê°€ê²© ìˆ˜ê¸° ë¹„ì„±ìˆ˜ê¸° ì£¼ì†Œ  ì¢€ ì¡°ê¸ˆ ë‹¤ìˆ˜ ëª‡ ì–¼ë§ˆ ì§€ë§Œ í•˜ë¬¼ë©° ë˜í•œ ê·¸ëŸ¬ë‚˜ ê·¸ë ‡ì§€ë§Œ í•˜ì§€ë§Œ ì´ì™¸ì—ë„ ëŒ€í•´ ë§í•˜ìë©´ ë¿ì´ë‹¤ ë‹¤ìŒì— ë°˜ëŒ€ë¡œ ë°˜ëŒ€ë¡œ ë§í•˜ìë©´ ì´ì™€ ë°˜ëŒ€ë¡œ ë°”ê¾¸ì–´ì„œ ë§í•˜ë©´ ë°”ê¾¸ì–´ì„œ í•œë‹¤ë©´ ë§Œì•½ ê·¸ë ‡ì§€ì•Šìœ¼ë©´ ê¹Œì•… íˆ­ ë”± ì‚ê±±ê±°ë¦¬ë‹¤ ë³´ë“œë“ ë¹„ê±±ê±°ë¦¬ë‹¤ ê½ˆë‹¹ ì‘ë‹¹ í•´ì•¼í•œë‹¤ ì— ê°€ì„œ ê° ê°ê° ì—¬ëŸ¬ë¶„ ê°ì¢… ê°ì ì œê°ê¸° í•˜ë„ë¡í•˜ë‹¤ ì™€ ê³¼ ê·¸ëŸ¬ë¯€ë¡œ ê·¸ë˜ì„œ ê³ ë¡œ í•œ ê¹Œë‹­ì— í•˜ê¸° ë•Œë¬¸ì— ê±°ë‹ˆì™€ ì´ì§€ë§Œ ëŒ€í•˜ì—¬ ê´€í•˜ì—¬ ê´€í•œ ê³¼ì—° ì‹¤ë¡œ ì•„ë‹ˆë‚˜ë‹¤ë¥¼ê°€ ìƒê°í•œëŒ€ë¡œ ì§„ì§œë¡œ í•œì ì´ìˆë‹¤ í•˜ê³¤í•˜ì˜€ë‹¤ í•˜ í•˜í•˜ í—ˆí—ˆ ì•„í•˜ ê±°ë°” ì™€ ì˜¤ ì™œ ì–´ì§¸ì„œ ë¬´ì—‡ë•Œë¬¸ì— ì–´ì°Œ í•˜ê² ëŠ”ê°€ ë¬´ìŠ¨ ì–´ë”” ì–´ëŠê³³ ë”êµ°ë‹¤ë‚˜ í•˜ë¬¼ë©° ë”ìš±ì´ëŠ” ì–´ëŠë•Œ ì–¸ì œ ì•¼ ì´ë´ ì–´ì´ ì—¬ë³´ì‹œì˜¤ íí í¥ íœ´ í—‰í—‰ í—ë–¡í—ë–¡ ì˜ì°¨ ì—¬ì°¨ ì–´ê¸°ì—¬ì°¨ ë™ë™ ì•„ì•¼ ì•— ì•„ì•¼ ì½¸ì½¸ ì¡¸ì¡¸ ì¢ì¢ ëšëš ì£¼ë£©ì£¼ë£© ì†¨ ìš°ë¥´ë¥´ ê·¸ë˜ë„ ë˜ ê·¸ë¦¬ê³  ë°”ê¾¸ì–´ë§í•˜ë©´ ë°”ê¾¸ì–´ë§í•˜ìë©´ í˜¹ì€ í˜¹ì‹œ ë‹µë‹¤ ë° ê·¸ì— ë”°ë¥´ëŠ” ë•Œê°€ ë˜ì–´ ì¦‰ ì§€ë“ ì§€ ì„¤ë ¹ ê°€ë ¹ í•˜ë”ë¼ë„ í• ì§€ë¼ë„ ì¼ì§€ë¼ë„ ì§€ë“ ì§€ ëª‡ ê±°ì˜ í•˜ë§ˆí„°ë©´ ì¸ì   ì´ì   ëœë°”ì—ì•¼ ëœì´ìƒ ë§Œí¼ ì–´ì°Œëë“  ê·¸ìœ„ì— ê²Œë‹¤ê°€ ì ì—ì„œ ë³´ì•„ ë¹„ì¶”ì–´ ë³´ì•„ ê³ ë ¤í•˜ë©´ í•˜ê²Œë ê²ƒì´ë‹¤ ì¼ê²ƒì´ë‹¤ ë¹„êµì  ì¢€ ë³´ë‹¤ë” ë¹„í•˜ë©´ ì‹œí‚¤ë‹¤ í•˜ê²Œí•˜ë‹¤ í• ë§Œí•˜ë‹¤ ì˜í•´ì„œ ì—°ì´ì„œ ì´ì–´ì„œ ì‡ë”°ë¼ í¬í•­ ìš°ë„ ì–‘ì–‘ ì „ì£¼ì‹œ í†µì˜ ì œì²œ ì—¬ìˆ˜ì‹œ ìˆœì²œ ê³ ì„± í•©ì²œ í•œë¦¼ì ì „ì£¼ ê²½ë¶ êµ¬ì¢Œì ëŒì‚°ì íƒœì•ˆ í•˜ë™ í¬í•­ì‹œ ì œì£¼ì‹œ ë°€ì–‘ ì–‘í‰ ìš¸ì‚° ë¬´ë¬´ ë’¤ë”°ë¼ ë’¤ì´ì–´ ê²°êµ­ ì˜ì§€í•˜ì—¬ ê¸°ëŒ€ì—¬ í†µí•˜ì—¬ ìë§ˆì ë”ìš±ë” ë¶ˆêµ¬í•˜ê³  ì–¼ë§ˆë“ ì§€ ë§ˆìŒëŒ€ë¡œ ì£¼ì €í•˜ì§€ ì•Šê³  ê³§ ì¦‰ì‹œ ë°”ë¡œ ë‹¹ì¥ í•˜ìë§ˆì ë°–ì— ì•ˆëœë‹¤ í•˜ë©´ëœë‹¤ ê·¸ë˜ ê·¸ë ‡ì§€ ìš”ì»¨ëŒ€ ë‹¤ì‹œ ë§í•˜ìë©´ ë°”ê¿” ì¶˜ì²œ ì¸µ ë‚¨í•´ ìŠ¤í…Œì´ ì„œìš¸ ì—¬ìˆ˜ ê±°ì œ ì¶”ì²œ ê²½ì£¼ ê³³ ê°ì‹¤ ì´ë‹¤ ê°•ì›ë„ ê°•ì› í™ì²œ ë¶€ì‚° ì˜ë„ í˜¸ ê°ì‹¤ êµë™ í’€ë¹Œë¼ ë¹Œë¼ í’€ ìŠ¤íŠœë””ì˜¤ íœì…˜ ë§í•˜ë©´ ì¦‰ êµ¬ì²´ì ìœ¼ë¡œ ë§í•˜ìë©´ ì‹œì‘í•˜ì—¬ ì‹œì´ˆì— ì´ìƒ í—ˆ í—‰ í—ˆê±± ë°”ì™€ê°™ì´ í•´ë„ì¢‹ë‹¤ í•´ë„ëœë‹¤ ê²Œë‹¤ê°€ ë”êµ¬ë‚˜ í•˜ë¬¼ë©° ì™€ë¥´ë¥´ íŒ í½ í„ë  ë™ì•ˆ ì´ë˜ í•˜ê³ ìˆì—ˆë‹¤ ì´ì—ˆë‹¤ ì—ì„œ ë¡œë¶€í„° ê¹Œì§€ ì˜ˆí•˜ë©´ í–ˆì–´ìš” í•´ìš” í•¨ê»˜ ê°™ì´ ë”ë¶ˆì–´ ë§ˆì € ë§ˆì €ë„ ì–‘ì ëª¨ë‘ ìŠµë‹ˆë‹¤ ê°€ê¹ŒìŠ¤ë¡œ í•˜ë ¤ê³ í•˜ë‹¤ ì¦ˆìŒí•˜ì—¬ ë‹¤ë¥¸ ë‹¤ë¥¸ ë°©ë©´ìœ¼ë¡œ í•´ë´ìš” ìŠµë‹ˆê¹Œ í–ˆì–´ìš” ë§í• ê²ƒë„ ì—†ê³  ë¬´ë¦ì“°ê³  ê°œì˜ì¹˜ì•Šê³  í•˜ëŠ”ê²ƒë§Œ ëª»í•˜ë‹¤ í•˜ëŠ”ê²ƒì´ ë‚«ë‹¤ ë§¤ ë§¤ë²ˆ ë“¤ ëª¨ ì–´ëŠê²ƒ ì–´ëŠ ë¡œì¨ ê°–ê³ ë§í•˜ìë©´ ì–´ë”” ì–´ëŠìª½ ì–´ëŠê²ƒ ì–´ëŠí•´ ì–´ëŠ ë…„ë„ ë¼ í•´ë„ ì–¸ì  ê°€ ì–´ë–¤ê²ƒ ì–´ëŠê²ƒ ì €ê¸° ì €ìª½ ì €ê²ƒ ê·¸ë•Œ ê·¸ëŸ¼ ê·¸ëŸ¬ë©´ ìš”ë§Œí•œê±¸ ê·¸ë˜ ê·¸ë•Œ ì €ê²ƒë§Œí¼ ê·¸ì € ì´ë¥´ê¸°ê¹Œì§€ í•  ì¤„ ì•ˆë‹¤ í•  í˜ì´ ìˆë‹¤ ë„ˆ ë„ˆí¬ ë‹¹ì‹  ì–´ì°Œ ì„¤ë§ˆ ì°¨ë¼ë¦¬ í• ì§€ì–¸ì • í• ì§€ë¼ë„ í• ë§ì • í• ì§€ì–¸ì • êµ¬í† í•˜ë‹¤ ê²Œìš°ë‹¤ í† í•˜ë‹¤ ë©”ì“°ê²ë‹¤ ì˜†ì‚¬ëŒ í‰¤ ì³‡ ì˜ê±°í•˜ì—¬ ê·¼ê±°í•˜ì—¬ ì˜í•´ ë”°ë¼ í˜ì…ì–´ ê·¸ ë‹¤ìŒ ë²„ê¸ˆ ë‘ë²ˆì§¸ë¡œ ê¸°íƒ€ ì²«ë²ˆì§¸ë¡œ ë‚˜ë¨¸ì§€ëŠ” ê·¸ì¤‘ì—ì„œ ê²¬ì§€ì—ì„œ í˜•ì‹ìœ¼ë¡œ ì“°ì—¬ ì…ì¥ì—ì„œ ìœ„í•´ì„œ ë‹¨ì§€ ì˜í•´ë˜ë‹¤ í•˜ë„ë¡ì‹œí‚¤ë‹¤ ë¿ë§Œì•„ë‹ˆë¼ ë°˜ëŒ€ë¡œ ì „í›„ ì „ì ì•ì˜ê²ƒ ì ì‹œ ì ê¹ í•˜ë©´ì„œ ê·¸ë ‡ì§€ë§Œ ë‹¤ìŒì— ê·¸ëŸ¬í•œì¦‰ ê·¸ëŸ°ì¦‰ ë‚¨ë“¤ ì•„ë¬´ê±°ë‚˜ ì–´ì°Œí•˜ë“ ì§€ ê°™ë‹¤ ë¹„ìŠ·í•˜ë‹¤ ì˜ˆì»¨ëŒ€ ì´ëŸ´ì •ë„ë¡œ ì–´ë–»ê²Œ ë§Œì•½ ë§Œì¼ ìœ„ì—ì„œ ì„œìˆ í•œë°”ì™€ê°™ì´ ì¸ ë“¯í•˜ë‹¤ í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë§Œì•½ì— ë¬´ì—‡ ë¬´ìŠ¨ ì–´ëŠ ì–´ë–¤ ì•„ë˜ìœ— ì¡°ì°¨ í•œë° ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ì—¬ì „íˆ ì‹¬ì§€ì–´ ê¹Œì§€ë„ ì¡°ì°¨ë„ í•˜ì§€ ì•Šë„ë¡ ì•Šê¸° ìœ„í•˜ì—¬ ë•Œ ì‹œê° ë¬´ë µ ì‹œê°„ ë™ì•ˆ ì–´ë•Œ ì–´ë– í•œ í•˜ì—¬ê¸ˆ ë„¤ ì˜ˆ ìš°ì„  ëˆ„êµ¬ ëˆ„ê°€ ì•Œê² ëŠ”ê°€ ì•„ë¬´ë„ ì¤„ì€ëª¨ë¥¸ë‹¤ ì¤„ì€ ëª°ëë‹¤ í•˜ëŠ” ê¹€ì— ê²¸ì‚¬ê²¸ì‚¬ í•˜ëŠ”ë°” ê·¸ëŸ° ê¹Œë‹­ì— í•œ ì´ìœ ëŠ” ê·¸ëŸ¬ë‹ˆ ê·¸ëŸ¬ë‹ˆê¹Œ ë•Œë¬¸ì— ê·¸ ë„ˆí¬ ê·¸ë“¤ ë„ˆí¬ë“¤ íƒ€ì¸ ê²ƒ ê²ƒë“¤ ë„ˆ ìœ„í•˜ì—¬ ê³µë™ìœ¼ë¡œ ë™ì‹œì— í•˜ê¸° ìœ„í•˜ì—¬ ì–´ì°Œí•˜ì—¬ ë¬´ì—‡ë•Œë¬¸ì— ë¶•ë¶• ìœ™ìœ™ ë‚˜ ìš°ë¦¬ ì—‰ì—‰ íœ˜ìµ ìœ™ìœ™ ì˜¤í˜¸ ì•„í•˜ ì–´ì¨‹ë“  ë§Œ ëª»í•˜ë‹¤ í•˜ê¸°ë³´ë‹¤ëŠ” ì°¨ë¼ë¦¬ í•˜ëŠ” í¸ì´ ë‚«ë‹¤ íí ë†€ë¼ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ë§í•˜ìë©´ ë§ˆì¹˜ ì•„ë‹ˆë¼ë©´ ì‰¿ ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ ì•ˆ ê·¸ëŸ¬ë©´ ì•„ë‹ˆì—ˆë‹¤ë©´ í•˜ë“ ì§€ ì•„ë‹ˆë©´ ì´ë¼ë©´ ì¢‹ì•„ ì•Œì•˜ì–´ í•˜ëŠ”ê²ƒë„ ê·¸ë§Œì´ë‹¤ ì–´ì©”ìˆ˜ ì—†ë‹¤ í•˜ë‚˜ ì¼ ì¼ë°˜ì ìœ¼ë¡œ ì¼ë‹¨ í•œì¼ ìœ¼ë¡œëŠ” ì˜¤ìë§ˆì ì´ë ‡ê²Œë˜ë©´ ì´ì™€ê°™ë‹¤ë©´ ì „ë¶€ í•œë§ˆë”” í•œí•­ëª© ê·¼ê±°ë¡œ í•˜ê¸°ì— ì•„ìš¸ëŸ¬ í•˜ì§€ ì•Šë„ë¡ ì•Šê¸° ìœ„í•´ì„œ ì´ë¥´ê¸°ê¹Œì§€ ì´ ë˜ë‹¤ ë¡œ ì¸í•˜ì—¬ ê¹Œë‹­ìœ¼ë¡œ ì´ìœ ë§Œìœ¼ë¡œ ì´ë¡œ ì¸í•˜ì—¬ ê·¸ë˜ì„œ ì´ ë•Œë¬¸ì— ê·¸ëŸ¬ë¯€ë¡œ ê·¸ëŸ° ê¹Œë‹­ì— ì•Œ ìˆ˜ ìˆë‹¤ ê²°ë¡ ì„ ë‚¼ ìˆ˜ ìˆë‹¤ ìœ¼ë¡œ ì¸í•˜ì—¬ ìˆë‹¤ ì–´ë–¤ê²ƒ ê´€ê³„ê°€ ìˆë‹¤ ê´€ë ¨ì´ ìˆë‹¤ ì—°ê´€ë˜ë‹¤ ì–´ë–¤ê²ƒë“¤ ì— ëŒ€í•´ ì´ë¦¬í•˜ì—¬ ê·¸ë¦¬í•˜ì—¬ ì—¬ë¶€ í•˜ê¸°ë³´ë‹¤ëŠ” í•˜ëŠë‹ˆ í•˜ë©´ í• ìˆ˜ë¡ ìš´ìš´ ì´ëŸ¬ì´ëŸ¬í•˜ë‹¤ í•˜êµ¬ë‚˜ í•˜ë„ë‹¤ ë‹¤ì‹œë§í•˜ë©´ ë‹¤ìŒìœ¼ë¡œ ì— ìˆë‹¤ ì— ë‹¬ë ¤ ìˆë‹¤ ìš°ë¦¬ ìš°ë¦¬ë“¤ ì˜¤íˆë ¤ í•˜ê¸°ëŠ”í•œë° ì–´ë–»ê²Œ ì–´ë–»í•´ ì–´ì°Œëì–´ ì–´ë•Œ ì–´ì§¸ì„œ ë³¸ëŒ€ë¡œ ì ì´ ì´ìª½ ì—¬ê¸° ì´ê²ƒ ì´ë²ˆ ì´ë ‡ê²Œë§í•˜ìë©´ ì´ëŸ° ì´ëŸ¬í•œ ì´ì™€ ê°™ì€ ìš”ë§Œí¼ ìš”ë§Œí•œ ê²ƒ ì–¼ë§ˆ ì•ˆ ë˜ëŠ” ê²ƒ ì´ë§Œí¼ ì´ ì •ë„ì˜ ì´ë ‡ê²Œ ë§ì€ ê²ƒ ì´ì™€ ê°™ë‹¤ ì´ë•Œ ì´ë ‡êµ¬ë‚˜ ê²ƒê³¼ ê°™ì´ ë¼ìµ ì‚ê±± ë”°ìœ„ ì™€ ê°™ì€ ì‚¬ëŒë“¤ ë¶€ë¥˜ì˜ ì‚¬ëŒë“¤ ì™œëƒí•˜ë©´ ì¤‘ì˜í•˜ë‚˜ ì˜¤ì§ ì˜¤ë¡œì§€ ì— í•œí•˜ë‹¤ í•˜ê¸°ë§Œ í•˜ë©´ ë„ì°©í•˜ë‹¤ ê¹Œì§€ ë¯¸ì¹˜ë‹¤ ë„ë‹¬í•˜ë‹¤ ì •ë„ì— ì´ë¥´ë‹¤ í•  ì§€ê²½ì´ë‹¤ ê²°ê³¼ì— ì´ë¥´ë‹¤ ê´€í•´ì„œëŠ” ì—¬ëŸ¬ë¶„ í•˜ê³  ìˆë‹¤ í•œ í›„ í˜¼ì ìê¸° ìê¸°ì§‘ ìì‹  ìš°ì— ì¢…í•©í•œê²ƒê³¼ê°™ì´ ì´ì ìœ¼ë¡œ ë³´ë©´ ì´ì ìœ¼ë¡œ ë§í•˜ë©´ ì´ì ìœ¼ë¡œ ëŒ€ë¡œ í•˜ë‹¤ ìœ¼ë¡œì„œ ì°¸ ê·¸ë§Œì´ë‹¤ í•  ë”°ë¦„ì´ë‹¤ ì¿µ íƒ•íƒ• ì¾…ì¾… ë‘¥ë‘¥ ë´ ë´ë¼ ì•„ì´ì•¼ ì•„ë‹ˆ ì™€ì•„ ì‘ ì•„ì´ ì°¸ë‚˜ ë…„ ì›” ì¼ ë ¹ ì˜ ì¼ ì´ ì‚¼ ì‚¬ ì˜¤ ìœ¡ ë¥™ ì¹  íŒ” êµ¬ ì´ì²œìœ¡ ì´ì²œì¹  ì´ì²œíŒ” ì´ì²œêµ¬ í•˜ë‚˜ ë‘˜ ì…‹ ë„· ë‹¤ì„¯ ì—¬ì„¯ ì¼ê³± ì—¬ëŸ ì•„í™‰ ë ¹ ì˜ ì´ ìˆ í•˜ ê²ƒ ë“¤ ê·¸ ë˜ ìˆ˜ ì´ ë³´ ì•Š ì—† ë‚˜ ì‚¬ëŒ ì£¼ ì•„ë‹ˆ ë“± ê°™ ìš°ë¦¬ ë•Œ ë…„ ê°€ í•œ ì§€ ëŒ€í•˜ ì˜¤ ë§ ì¼ ê·¸ë ‡ ìœ„í•˜ ë•Œë¬¸ ê·¸ê²ƒ ë‘ ë§í•˜ ì•Œ ê·¸ëŸ¬ë‚˜ ë°› ëª»í•˜ ì¼ ê·¸ëŸ° ë˜ ë¬¸ì œ ë” ì‚¬íšŒ ë§ ê·¸ë¦¬ê³  ì¢‹ í¬ ë”°ë¥´ ì¤‘ ë‚˜ì˜¤ ê°€ì§€ ì”¨ ì‹œí‚¤ ë§Œë“¤ ì§€ê¸ˆ ìƒê°í•˜ ê·¸ëŸ¬ ì† í•˜ë‚˜ ì§‘ ì‚´ ëª¨ë¥´ ì  ì›” ë° ìì‹  ì•ˆ ì–´ë–¤ ë‚´ ë‚´ ê²½ìš° ê°ì„± ìˆ™ì†Œ í˜¸í…” ì œì£¼ ì œì£¼ë„ ê°•ì›ë„ ê°•ë¦‰ ì†ì´ˆ ì†ì´ˆì‹œ ì—ì–´ë¹„ì•¤ë¹„ ëª… ìƒê° ì‹œê°„ ê·¸ë…€ ìˆ˜ ì•½ ë‹¤ì‹œ ì´ëŸ° ì• ë³´ì´ ë²ˆ ë‚˜ ë‹¤ë¥¸ ì–´ë–» ì—¬ì ê°œ ì „ ë“¤ ì‚¬ì‹¤ ì´ë ‡ ì  ì‹¶ ë§ ì •ë„ ì¢€ ì› ì˜ í†µí•˜ ë†“ ì‚¬ì‹¤ ì´ë ‡ ì  ì‹¶ ë§ ì •ë„ ì¢€ ì› ì˜ í†µí•˜ ã…† ì¬ ì±„ ë… ë¡­ ë„ë€ë„ íŒ… ê°ì„±ë¹„ì•¤ë¹„ ê³µê°„ ì°¸ì—¬ ì´ë²¤íŠ¸ ë‹¹ì²¨ì ê¸°ê°„ ë¶€ì‚° #ë¶€ì‚°ìˆ™ì†Œ #ë¶€ì‚°ì—ì–´ë¹„ì•¤ë¹„ #ë¶€ì‚°ì—¬í–‰ ê²½ê¸° ê²½ì£¼ì‹œ ìˆëŠ” í•œ #ê²½ì£¼ìˆ™ì†Œ ê°•ì›ë„ ì œì£¼ ì œì£¼ì‹œ #ì œì£¼ìˆ™ì†Œ #ì œì£¼ê°ì„±ìˆ™ì†Œ #ì œì£¼ìˆ™ì†Œì¶”ì²œ ë‚¨í•´êµ° ì„œë©´ ì¶˜ì²œì‹œ ë‚¨í•´ ê´‘ì•ˆë¦¬ í¸ ë°˜ ì•ˆë©´ë„ ì‚°ë°©ì‚° ë‹˜ ì‚¬ì¥ ì •ë§ ê³µì£¼ ì•ˆë™ ì¶©ë‚¨ ì „ë¼ë¶ë„ ì¶©ì²­ë‚¨ë„ ì „ë¶ ì‹¤ì‹œê°„ í‡´ í™œìš© ìƒí’ˆê¶Œ ì´ë©”ì¼ ê²½ìƒë‚¨ë„ ë‚  ê¼­ ì°¾ë‹¤ ì§ì ‘ ë‚  ê·¸ëƒ¥ ë§˜ ê·¼ë° ìŠ¤íƒ€ ìš• í¸ ì§¸ ì²« ë§› ì±„ í™©ë¦¬ë‹¨ í™©ë¦¬ë‹¨ê¸¸ ë¯¸ë¦¬ ì—´ë‹¤ ê±°ì œë„ ê³µê°„ ê´‘ì•ˆë¦¬ ê´‘ì•ˆëŒ€êµ ìŠ¤ë‹¤ í•´ìš´ëŒ€ í—¤ì´ ìœ¤ìŠ¬ ìš¸ë¦‰ë„ ì† ì˜† í™ë³´ ì…ì‹¤ì‹œê°„ ì—ë””í„°ê²½ê¸°ë„ ìº í•‘ ì²­ì£¼ ê°€í‰ ì²­ë„ í¬ì²œ í•‘ ê°•í™”ë„ ê·¼êµ ì¸ì²œ' #ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ í˜•ì„±
    stop_words = stop_words.split(' ')
    stop_location = pd.read_csv('/home/ubuntu/DS/location_words.csv')
    stop_location.columns = ['index', 'location']
    stop_location = stop_location.location.tolist()

    stop = stop_words + stop_location + words
    stop = pd.Series(stop)
    stopwords = stop.unique().tolist()

    return stopwords

# í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ ì •ì˜ 


# tokenize í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. í•œêµ­ì–´ ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³ , 
# ë¶ˆìš©ì–´ ë° íŠ¹ìˆ˜ ë¬¸ì ë“±ì„ ì œê±°í•œ ë’¤, listë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

def make_userdic(input_text):
    with open (input_text, 'r', encoding= 'utf-8') as f:
        words = f.readline().split()
    kiwi.add_user_word(words) # ì‚¬ìš©ìì‚¬ì „


# tokenize í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. í•œêµ­ì–´ ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³ , 
# ë¶ˆìš©ì–´ ë° íŠ¹ìˆ˜ ë¬¸ì ë“±ì„ ì œê±°í•œ ë’¤, listë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

def tokenize(sent):
    from kiwipiepy import Kiwi
    stoptext = '/home/ubuntu/DS/stopwords_korean.txt'
    stopwords = make_stopwords(stoptext)

    user_word= 'í˜¸ìº‰ìŠ¤  í˜¸í…”  íë§  ë…ì±„ìˆ™ì†Œ  ë…ì±„  í•œì˜¥  ìì—°  ì˜¤ì…˜ë·° ì™¸êµ­ ìº í•‘ ë°”ë‹¤ íŒŒí‹° ëŸ½ìŠ¤íƒ€ ì–´ë©”ë‹ˆí‹° ë¹”í”„ë¡œì íŠ¸ ë¹” í”„ë¡œì íŠ¸ ê¸€ë¨í•‘ í’€ë¹Œë¼ ë…¸ì²œíƒ• í”„ë¼ì´ë¹— ì¸í”¼ë‹ˆí‹°í’€ ë£¨í”„íƒ‘ íˆë…¸ë¼íƒ• íˆë…¸ë¼ ì˜¨ìˆ˜í’€ ìˆ˜ì˜ì¥ ìˆ²ì† ìˆ² ë‚˜ë¬´'
    # naver = './total_blog_dataset.csv'
    kiwi = Kiwi()
    kiwi.add_user_word(user_word, 'NNP', 10)
    kiwi.prepare()
    time.sleep(5)

    res, score = kiwi.analyze(sent)[0] # ì²«ë²ˆì§¸ ê²°ê³¼ë¥¼ ì‚¬ìš©
    return [word + ('ë‹¤' if tag.startswith('V') else '') # ë™ì‚¬ì—ëŠ” 'ë‹¤'ë¥¼ ë¶™ì—¬ì¤Œ
            for word, tag, _, _ in res
            if not tag.startswith('E') and not tag.startswith('J') and not tag.startswith('S') and not tag.startswith('W') and word not in stopwords] # ì¡°ì‚¬, ì–´ë¯¸, íŠ¹ìˆ˜ê¸°í˜¸ ë° stopwordsì— í¬í•¨ëœ ë‹¨ì–´ëŠ” ì œê±°


# def tokenize_nouns(sent):
#     res, score = kiwi.analyze(sent)[0] # ì²«ë²ˆì§¸ ê²°ê³¼ë¥¼ ì‚¬ìš©
#     return [word + ('ë‹¤' if tag.startswith('V') else '') # ë™ì‚¬ì—ëŠ” 'ë‹¤'ë¥¼ ë¶™ì—¬ì¤Œ
#             for word, tag, _, _ in res
#             if tag.startswith('N')] # ì¡°ì‚¬, ì–´ë¯¸, íŠ¹ìˆ˜ê¸°í˜¸ ë° stopwordsì— í¬í•¨ëœ ë‹¨ì–´ëŠ” ì œê±°


"""ì¸ìŠ¤íƒ€ ë°ì´í„° ì „ì²˜ë¦¬"""

#"""í•œê¸€ë¹¼ê³  ì „ë¶€ ì œê±°"""
def sub_special(s):
    rs = re.sub(r'[^ê°€-í£]',' ',s)
    rr = re.sub(' +', ' ', rs)
    return rr

#"""í•œê¸€ë¹¼ê³  ì „ë¶€ ì œê±°"""
def sub_special_token(s):
    rs = re.sub(r'[^ê°€-í£]',' ',s).strip().split()
    return rs

#ë¶ˆìš©ì–´ ì œê±°
def word_cleansing(data): 
    stoptext = '/home/ubuntu/DS/stopwords_korean.txt'
    stopwords = make_stopwords(stoptext)
    for i in range(len(data)): 
        result = []
        for w in data.content[i]:
            if w not in stopwords:
                result.append(w)
    
        data.content[i] = result
    return data
        
def make_total_list(naver_data, insta_data):
    n_data = pd.DataFrame(naver_data.copy()) # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„° ë¡œë“œ(ì½˜í…ì¸ )
    i_data = pd.DataFrame(insta_data[['index', 'content']].copy()) # ì¸ìŠ¤íƒ€ ë°ì´í„° ë¡œë“œ(ì¸ë±ìŠ¤, ì½˜í…ì¸ )

    i_data.columns = ['item_idx', 'content'] # mergeë¥¼ ìœ„í•œ ì‹ë³„ì ì»¬ëŸ¼('item_idx') ìƒì„±

    all_items = pd.concat([i_data, n_data]) # ë‘ ë°ì´í„° í–‰ ë³‘í•©

    all_items.reset_index(drop=True, inplace=True) #ì¸ë±ìŠ¤ ë¦¬ì…‹

    #ë¹ˆ ìƒ˜í”Œ í™•ì¸ ë° ì œê±°
    drop = [index for index in range(len(all_items)) if len(all_items.content[index]) < 1] # dropì— empty ìƒ˜í”Œì˜ ì¸ë±ìŠ¤ë¥¼ ì €ì¥


    while len(drop) > 0: 
        # ë¹ˆ ìƒ˜í”Œ ì œê±°
        for i in drop:
            all_items.drop(all_items.index[i], inplace=True)
        # ì¸ë±ìŠ¤ ë¦¬ì…‹
        all_items.reset_index(drop=True, inplace=True)

        drop = [index for index in range(len(all_items)) if len(all_items.content[index]) < 1] # ì¬í™•ì¸ 

     # ì¸ë±ìŠ¤ ë¦¬ì…‹
    all_items.reset_index(drop=True, inplace=True) #ìµœì¢… ë¦¬ì…‹

    return all_items



def create_model(data, clusters_n):
    word_vectors = data['wv'].to_list() 
    num_clusters = clusters_n

    # K means ë¥¼ ì •ì˜í•˜ê³  í•™ìŠµì‹œí‚¨ë‹¤.
    kmeans_clustering = KMeans(n_clusters=num_clusters, init='k-means++', n_init=25, max_iter = 600, random_state=0)
    idx = kmeans_clustering.fit_predict( word_vectors )
    data['category'] = idx
    
def get_sentence_mean_vector(morphs):
    # from gensim.models import Word2Vec
    # embedding_model = Word2Vec(new_items.content, vector_size=100, window = 2, min_count=3, workers=4, epochs=200, sg=1, seed=0)
    vector = []
    for i in morphs:
        try:
            vector.append(embedding_model.wv[i])
        except KeyError as e:
            pass
    try:
        return sum(vector)/len(vector)
    except IndexError as e:
        pass
    
def token_dummy(docs):
    return docs

def embedding(data):
    embedding_model = Word2Vec(data.content, vector_size=100, window = 2, min_count=3, workers=4, epochs=100, sg=1)
    return embedding_model

def keyword_extractor():
    keyword_extractor = KeywordSummarizer(tokenize = token_dummy,
    min_count=5,
    window=8,                     # cooccurrence within a sentence
    min_cooccurrence=3,
    vocab_to_idx=None,             # you can specify vocabulary to build word graph
    df=0.85,                       # PageRank damping factor
    max_iter=30,                   # PageRank maximum iteration
    verbose=False)
    
    return keyword_extractor

def cos_sim(A, B):
       return dot(A, B)/(norm(A)*norm(B))

    
def create_matrix(data):
    wv_matrix = np.asarray(data.wv)
    rows = []
    matrix = []
    for i in range(len(data.content)):
        for x in range(len(data.content)):
            cos_sim = dot(wv_matrix[i], wv_matrix[x])/(norm(wv_matrix[i])*norm(wv_matrix[x]))
            rows.append(cos_sim)
        matrix.append(rows)
        rows=[]
    return matrix

def create_model_spherical(data, n_clusters):
    x = data.wv.to_list()
    x = np.asarray(x)
    csr = csr_matrix(x)
    spherical_kmeans = SphericalKMeans(
    n_clusters=n_clusters,
    max_iter=15,
    verbose=1,
    init='similar_cut')
    time.sleep(1)
    index = spherical_kmeans.fit_predict(csr)
    data['category'] = index
    
def token(docs):
    return docs

def data_embedding(wv_data):
    data_embedding = []
    for i in wv_data:
        data_embedding.append(i)
    data_vector = np.array(data_embedding)
    
    return data_vector

def create_data_matrix(data_vector, item_vector, basedata):
    item_vector = np.asarray(item_vector)
    row = []
    data_matrix = []
    for i in range(len(data_vector)):
        for x in range(len(data.content)):
            cos_sim = dot(data_vector[i], item_vector[x])/(norm(data_vector[i])*norm(item_vector[x]))
            rows.append(cos_sim)
        matrix.append(rows)
        rows=[]
   
    return user_matrix

def data_recommand(data_matrix, basedata):
    matrix = np.array(data_matrix).reshape(-1)
    scores = np.argsort(-matrix)[:100]
    result = basedata.loc[scores, :]
    
    return result

def category_infer(result): #ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¶”ì²œ ëª©ë¡ì„ ì¸í’‹ê°’ìœ¼ë¡œ í™œìš©í•¨
    count = dict(result.category.value_counts())
    for k,v in count.items():
        a = []
        a.append(k)
        new_category = a[0]
        
    return new_category

def create_new_matrix(newdata, basedata):
    item_vector = np.asarray(basedata.wv)
    data_vector = np.array(newdata.wv.to_list())
    rows = []
    matrix = []
    for i in range(len(data_vector)):
        for x in range(len(basedata.content)):
            cos_sim = dot(data_vector[i], item_vector[x])/(norm(data_vector[i])*norm(item_vector[x]))
            rows.append(cos_sim)
        matrix.append(rows)
        rows=[]
    return matrix

def match_category(matrix, newdata, basedata):
    matrix = np.array(matrix)
    scores = np.argsort(-matrix)
    total = []
    for x in range(len(newdata)):
        index = []
        for i in range(100):
            temp = np.where(scores[x] == i)[0][0]
            index.append(temp)
            result = basedata.loc[index, :]
            category = category_infer(result)
        total.append(category)
    newdata['category'] = total
    
    return newdata


#######################################################################################################
# ë°ì´í„° ë¡œë“œ


def data_load():
    from kiwipiepy import Kiwi
    import pandas as pd
    import numpy as np
    import re
    import time
    from kiwipiepy import Kiwi

    from pyspark.conf import SparkConf
    from pyspark.sql import SparkSession
    from pyspark.sql.types import StructType,StructField, StringType, IntegerType
    from pyspark.sql.functions import array_contains, udf
    from datetime import datetime

    from gensim.models import Word2Vec
    from numpy import dot
    from numpy.linalg import norm
    import numpy as np
    from scipy.sparse import csr_matrix

    from gensim.models import Word2Vec


    nowtime = datetime.today().strftime("%Y-%m-%d")
    spark = SparkSession        .builder        .appName('aaa')        .getOrCreate()

    base_data = spark.read.parquet(f"hdfs://localhost:9000/data/modeldata/tot_dataset_2021-10-04.parquet")
    base_data = base_data.toPandas()


    new_data = spark.read.parquet(f"hdfs://localhost:9000/data/modeldata/new_{nowtime}.parquet")
    new_data = new_data.toPandas()

    new_data.reset_index(drop = True, inplace=True)
    original = new_data.content.copy()

    ####################################################################################################

    #í˜•íƒœì†Œ ë¶„ì„ê¸° / ë¶ˆìš©ì–´ë¦¬ìŠ¤íŠ¸ ìƒì„±
    stoptext = '/home/ubuntu/DS/stopwords_korean.txt'
    stopwords = make_stopwords(stoptext) #ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    # user_word= 'í˜¸ìº‰ìŠ¤  í˜¸í…”  íë§  ë…ì±„ìˆ™ì†Œ  ë…ì±„  í•œì˜¥  ìì—°  ì˜¤ì…˜ë·° ì™¸êµ­ ìº í•‘ ë°”ë‹¤ íŒŒí‹° ëŸ½ìŠ¤íƒ€ ì–´ë©”ë‹ˆí‹° ë¹”í”„ë¡œì íŠ¸ ë¹” í”„ë¡œì íŠ¸ ê¸€ë¨í•‘ í’€ë¹Œë¼ ë…¸ì²œíƒ• í”„ë¼ì´ë¹— ì¸í”¼ë‹ˆí‹°í’€ ë£¨í”„íƒ‘ íˆë…¸ë¼íƒ• íˆë…¸ë¼ ì˜¨ìˆ˜í’€ ìˆ˜ì˜ì¥ ìˆ²ì† ìˆ² ë‚˜ë¬´'
    # naver = './total_blog_dataset.csv'
    # kiwi = Kiwi()
    # kiwi.add_user_word(user_word, 'NNP', 10)
    # kiwi.prepare()
    # time.sleep(10)
    # 1ì°¨ í´ë¦°ì§•(ì˜ì–´, íŠ¹ìˆ˜ë¬¸ì, ìˆ«ìì œê±°)
    for i in range(len(new_data.content)):
        new_data.content[i] = sub_special(new_data.content[i])
    time.sleep(10)
    # í† í°í™”
    for i in range(len(new_data.content)):
        new_data.content[i]= tokenize(new_data.content[i])
    time.sleep(10)
    # 2ì°¨ í´ë¦°ì§•(ë¶ˆìš©ì–´ ì²˜ë¦¬)
    word_cleansing(new_data)

    time.sleep(1)

    new_items = new_data.copy()
    time.sleep(5)
    global embedding_model
    embedding_model = Word2Vec(new_items.content, vector_size=100, window = 2, min_count=3, workers=4, epochs=200, sg=1, seed=0)
    time.sleep(5)
    new_items['wv'] = new_items['content'].map(get_sentence_mean_vector)

    time.sleep(1)

    # data_vector = data_embedding(new_items.wv)
    # data_matrix = create_data_matrix(data_vector, new_items.wv, new_items)
    # result = data_recommand(data_matrix, new_items)

    # new_category = category_infer(result)

    matrix = create_new_matrix(new_items, base_data)
    new_data = match_category(matrix, new_items, base_data)


    # time.sleep(1)
    new_data.drop('__index_level_0__', axis = 1, inplace = True)
    new_data['original_content'] = original
    new_data['category_label'] = ' '

    new_data.to_parquet(f'/home/ubuntu/DE/new_data_{nowtime}.parquet')


def newdata_to_hdfs_fin():
    import subprocess
    # move file to hdfs.
    nowtime = datetime.today().strftime("%Y-%m-%d")
    load_to_hadoop_script = "hdfs dfs -moveFromLocal {0} {1}".format('/home/ubuntu/DE/new_data_' + nowtime + '.parquet', '/data/modeldata')
    subprocess.call(load_to_hadoop_script,shell=True)


# hdfs -> mongoDB ì´ë™ newdata_to_mongo
def newdata_to_mongo():
    from pyspark.conf import SparkConf 
    from pyspark.sql import SparkSession
    from pyspark.sql.types import StructType,StructField, StringType, IntegerType
    import pyspark
    import pyspark.sql.functions as f 

    from pymongo import MongoClient
    from datetime import datetime
    import json

    # pymongo connect
    client = MongoClient('localhost',27017) # mongodb 27017 port
    db = client.ojo_db

    spark = SparkSession.builder.appName("example-pyspark-read-and-write").getOrCreate()
    nowtime = datetime.today().strftime('%Y-%m-%d')

    newdata = spark.read.parquet(f"hdfs://localhost:9000/data/modeldata/new_data_{nowtime}.parquet")
  
    for i in newdata.collect():
        db.cluster.insert_one(i.asDict())

    



####################
# Dag Task Setting
####################

csv_to_parquet = PythonOperator(
        task_id = 'csvToparquet',
        python_callable = insta_call,       
        #provide_context=True,
        dag = dag
        )
        

parquet_load_to_HDFS = PythonOperator(
    task_id = 'parquet_move',
    python_callable = load_to_hdfs,
    dag = dag
    )


new_data_create = PythonOperator(
    task_id = 'create_newdata',
    python_callable = insta_spark_processing,       
    #provide_context=True,
    dag = dag
    )

# hdfs ì ì¬
newdata_hdfs = PythonOperator(
    task_id = 'newdata_parquet_move',
    python_callable = newdata_to_hdfs,       
    #provide_context=True,
    dag = dag
    )

# new data ë°ì´í„° ë¡œë“œ
newdata_load = PythonOperator(
    task_id = 'newdata_load',
    python_callable = data_load,       
    #provide_context=True,
    dag = dag
    )

# newdata_to_hdfs
newdata_hdfs_load = PythonOperator(
    task_id = 'newdata_hdfs_load',
    python_callable = newdata_to_hdfs_fin,       
    #provide_context=True,
    dag = dag
    )


# hdfs -> mongoDB ì´ë™
newdata_mongo_move = PythonOperator(
    task_id = 'newdata_mongo_move',
    python_callable = newdata_to_mongo,       
    #provide_context=True,
    dag = dag
    )

#Assign the order of the tasks in our DAG
csv_to_parquet >> parquet_load_to_HDFS >> new_data_create >> newdata_hdfs >> newdata_load >> newdata_hdfs_load >> newdata_mongo_move